{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Lgh6vORoHFg-"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_data_dir = '/content/train1'\n",
        "test_data_dir = '/content/validation1'"
      ],
      "metadata": {
        "id": "2kL8IkkoHNpL"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_shape = (150, 150, 3)\n",
        "num_classes = 6"
      ],
      "metadata": {
        "id": "xIUl-Kd1TPey"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model.add(Conv2D(32, (3, 3), activation='relu', input_shape=input_shape))\n",
        "model.add(MaxPooling2D((2, 2)))\n",
        "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
        "model.add(MaxPooling2D((2, 2)))\n",
        "model.add(Conv2D(128, (3, 3), activation='relu'))\n",
        "model.add(MaxPooling2D((2, 2)))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dense(num_classes, activation='softmax'))"
      ],
      "metadata": {
        "id": "D3CzNp8gTXPA"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "kudJGALWTYQP"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_datagen = ImageDataGenerator(rescale=1./255, shear_range=0.2, zoom_range=0.2, horizontal_flip=True)\n",
        "#train_generator = train_datagen.flow_from_directory(train_data_dir, target_size=input_shape[:2], batch_size=32, class_mode='categorical')\n",
        "train_generator = train_datagen.flow_from_directory(train_data_dir, target_size=input_shape[:2], batch_size=32, class_mode='categorical', classes=list(train_generator.class_indices.keys())[1:])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fD44r9QBThUF",
        "outputId": "9297b554-6aa7-4193-eeac-909205c3cdcc"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 1495 images belonging to 6 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_generator.class_indices)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3leKEV60e2IX",
        "outputId": "1bf61de6-8dae-40d1-a674-9c6caea6f320"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'.ipynb_checkpoints': 0, 'Skintrim.N': 1, 'Swizzor.gen!E': 2, 'Swizzor.gen!I': 3, 'VB.AT': 4, 'Wintrim.BX': 5, 'Yuner.A': 6}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "valid_datagen = ImageDataGenerator(rescale=1./255)\n",
        "#valid_generator = valid_datagen.flow_from_directory(test_data_dir, target_size=input_shape[:2], batch_size=32, class_mode='categorical')\n",
        "# Exclude '.ipynb_checkpoints' class from the generator\n",
        "valid_generator = valid_datagen.flow_from_directory(test_data_dir, target_size=input_shape[:2], batch_size=32, class_mode='categorical', classes=list(valid_generator.class_indices.keys())[1:])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kqruZ1HXcPhk",
        "outputId": "eb999e17-104a-4e54-a3e6-cf5cb46f0626"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 150 images belonging to 6 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(valid_generator.class_indices)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sZriDS-HfiJ9",
        "outputId": "d0e24058-4c6f-41ee-f8a4-05ed0e476ded"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'.ipynb_checkpoints': 0, 'Skintrim.N': 1, 'Swizzor.gen!E': 2, 'Swizzor.gen!I': 3, 'VB.AT': 4, 'Wintrim.BX': 5, 'Yuner.A': 6}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qjb8PLueflPJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(train_generator, epochs=10, validation_data=valid_generator)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jyIP19jOTyy4",
        "outputId": "a5ad217a-318e-4bfd-fd7c-a94cc39a4100"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "47/47 [==============================] - 56s 1s/step - loss: 0.7281 - accuracy: 0.7860 - val_loss: 2.1027 - val_accuracy: 0.5600\n",
            "Epoch 2/10\n",
            "47/47 [==============================] - 57s 1s/step - loss: 0.2188 - accuracy: 0.9151 - val_loss: 6.0192 - val_accuracy: 0.1800\n",
            "Epoch 3/10\n",
            "47/47 [==============================] - 55s 1s/step - loss: 0.1461 - accuracy: 0.9344 - val_loss: 2.3219 - val_accuracy: 0.2600\n",
            "Epoch 4/10\n",
            "47/47 [==============================] - 55s 1s/step - loss: 0.1414 - accuracy: 0.9371 - val_loss: 6.3056 - val_accuracy: 0.2067\n",
            "Epoch 5/10\n",
            "47/47 [==============================] - 55s 1s/step - loss: 0.1309 - accuracy: 0.9405 - val_loss: 12.7182 - val_accuracy: 0.1667\n",
            "Epoch 6/10\n",
            "47/47 [==============================] - 56s 1s/step - loss: 0.1320 - accuracy: 0.9425 - val_loss: 4.5877 - val_accuracy: 0.5333\n",
            "Epoch 7/10\n",
            "47/47 [==============================] - 62s 1s/step - loss: 0.0990 - accuracy: 0.9572 - val_loss: 3.4813 - val_accuracy: 0.4667\n",
            "Epoch 8/10\n",
            "47/47 [==============================] - 58s 1s/step - loss: 0.1173 - accuracy: 0.9498 - val_loss: 0.2706 - val_accuracy: 0.8333\n",
            "Epoch 9/10\n",
            "47/47 [==============================] - 57s 1s/step - loss: 0.0961 - accuracy: 0.9579 - val_loss: 5.5091 - val_accuracy: 0.1867\n",
            "Epoch 10/10\n",
            "47/47 [==============================] - 56s 1s/step - loss: 0.0853 - accuracy: 0.9565 - val_loss: 2.8776 - val_accuracy: 0.3667\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f468d8d5c90>"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    }
  ]
}